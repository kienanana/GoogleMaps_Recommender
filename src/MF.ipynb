{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"../data/processed\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2871735, 444233, 968857)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load interactions\n",
    "reviews_path = DATA / \"ny_food_reviews.parquet\"\n",
    "df = pd.read_parquet(reviews_path)\n",
    "\n",
    "def temporal_split(df):\n",
    "    df = df.dropna(subset=[\"user_id\",\"gmap_id\",\"rating\",\"time\"]).copy()\n",
    "    df = df.sort_values([\"user_id\",\"time\"])\n",
    "    tri, vai, tei = [], [], []\n",
    "    for uid, g in df.groupby(\"user_id\", sort=False):\n",
    "        n = len(g)\n",
    "        if n < 5:  # skip ultra-short histories\n",
    "            continue\n",
    "        t1, t2 = int(0.7*n), int(0.8*n)\n",
    "        tri.append(g.iloc[:t1]); vai.append(g.iloc[t1:t2]); tei.append(g.iloc[t2:])\n",
    "    tr = pd.concat(tri); va = pd.concat(vai); te = pd.concat(tei)\n",
    "    return tr, va, te\n",
    "\n",
    "split_files = [DATA/\"ny_train.parquet\", DATA/\"ny_val.parquet\", DATA/\"ny_test.parquet\"]\n",
    "if all(p.exists() for p in split_files):\n",
    "    train_df = pd.read_parquet(split_files[0])\n",
    "    val_df   = pd.read_parquet(split_files[1])\n",
    "    test_df  = pd.read_parquet(split_files[2])\n",
    "else:\n",
    "    train_df, val_df, test_df = temporal_split(df)\n",
    "    train_df.to_parquet(split_files[0], index=False)\n",
    "    val_df.to_parquet(split_files[1], index=False)\n",
    "    test_df.to_parquet(split_files[2], index=False)\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lite sampling:\n",
      " users: 5000 | items: 18732\n",
      " train/val/test sizes: 107519 15050 30554\n"
     ]
    }
   ],
   "source": [
    "### sampling\n",
    "SEED = 42\n",
    "MIN_USER_HIST = 10     \n",
    "SAMPLE_USERS = None     # None for now\n",
    "MAX_ITEMS = 30000       \n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# 1) filter users by history in TRAIN (so train/val/test remain consistent)\n",
    "hist = train_df.groupby(\"user_id\")[\"gmap_id\"].count()\n",
    "eligible_users = hist[hist >= MIN_USER_HIST].index\n",
    "\n",
    "if SAMPLE_USERS is not None and len(eligible_users) > SAMPLE_USERS:\n",
    "    sampled_users = pd.Index(rng.choice(eligible_users.values, size=SAMPLE_USERS, replace=False))\n",
    "else:\n",
    "    sampled_users = eligible_users\n",
    "\n",
    "train_df = train_df[train_df.user_id.isin(sampled_users)].copy()\n",
    "val_df   = val_df[val_df.user_id.isin(sampled_users)].copy()\n",
    "test_df  = test_df[test_df.user_id.isin(sampled_users)].copy()\n",
    "\n",
    "if MAX_ITEMS is not None:\n",
    "    pop_items = train_df.groupby(\"gmap_id\")[\"user_id\"].count().sort_values(ascending=False)\n",
    "    keep_items = pop_items.index[:MAX_ITEMS]\n",
    "    train_df = train_df[train_df.gmap_id.isin(keep_items)].copy()\n",
    "    val_df   = val_df[val_df.gmap_id.isin(keep_items)].copy()\n",
    "    test_df  = test_df[test_df.gmap_id.isin(keep_items)].copy()\n",
    "\n",
    "print(\"After lite sampling:\")\n",
    "print(\" users:\", train_df.user_id.nunique(), \"| items:\", train_df.gmap_id.nunique())\n",
    "print(\" train/val/test sizes:\", len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 18732)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### build id maps from ALL splits to avoid unknown ids at test time\n",
    "all_users = pd.Index(pd.concat([train_df.user_id, val_df.user_id, test_df.user_id]).unique())\n",
    "all_items = pd.Index(pd.concat([train_df.gmap_id, val_df.gmap_id, test_df.gmap_id]).unique())\n",
    "u2i = {u:i for i,u in enumerate(all_users)}\n",
    "v2i = {v:i for i,v in enumerate(all_items)}\n",
    "\n",
    "def encode(df):\n",
    "    df = df.copy()\n",
    "    df[\"u\"] = df.user_id.map(u2i)\n",
    "    df[\"i\"] = df.gmap_id.map(v2i)\n",
    "    df[\"r\"] = df.rating.astype(\"float32\")\n",
    "    return df.dropna(subset=[\"u\",\"i\"])\n",
    "\n",
    "train_e = encode(train_df)\n",
    "val_e   = encode(val_df)\n",
    "n_users, n_items = len(all_users), len(all_items)\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch dataset and model\n",
    "class RatingDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.u = torch.tensor(df.u.values, dtype=torch.long)\n",
    "        self.i = torch.tensor(df.i.values, dtype=torch.long)\n",
    "        self.r = torch.tensor(df.r.values, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.u)\n",
    "    def __getitem__(self, idx): return self.u[idx], self.i[idx], self.r[idx]\n",
    "\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, k=64, bias=True):\n",
    "        super().__init__()\n",
    "        self.P = nn.Embedding(n_users, k)\n",
    "        self.Q = nn.Embedding(n_items, k)\n",
    "        nn.init.normal_(self.P.weight, std=0.01)\n",
    "        nn.init.normal_(self.Q.weight, std=0.01)\n",
    "        self.bias = bias\n",
    "        if bias:\n",
    "            self.ub = nn.Embedding(n_users, 1)\n",
    "            self.ib = nn.Embedding(n_items, 1)\n",
    "            self.mu = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, u, i):\n",
    "        s = (self.P(u) * self.Q(i)).sum(-1)\n",
    "        if self.bias:\n",
    "            s = s + self.ub(u).squeeze(-1) + self.ib(i).squeeze(-1) + self.mu\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train MSE=18.6812\n",
      "Epoch 2: train MSE=13.1024\n",
      "Epoch 3: train MSE=5.2518\n",
      "Epoch 4: train MSE=1.7436\n",
      "Epoch 5: train MSE=1.0950\n",
      "Epoch 6: train MSE=0.9342\n"
     ]
    }
   ],
   "source": [
    "### TRAIN MF (MSE on explicit ratings)\n",
    "batch_size=4096; epochs=6; k=64\n",
    "model = MF(n_users, n_items, k=k, bias=True).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "dl = DataLoader(RatingDS(train_e), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train(); total=0\n",
    "    for u,i,r in dl:\n",
    "        u,i,r = u.to(device), i.to(device), r.to(device)\n",
    "        pred = model(u,i)\n",
    "        loss = loss_fn(pred, r)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        total += loss.item()*len(u)\n",
    "    print(f\"Epoch {ep+1}: train MSE={total/len(train_e):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate top-200 candidates (base_score)\n",
    "@torch.no_grad()\n",
    "def topk_candidates_streaming(model, K=200, users_idx=None):\n",
    "    \"\"\"\n",
    "    Memory-safe: for each user u, compute scores = Q @ P[u] (vector-matrix),\n",
    "    then take top-K. Avoids building full P @ Q^T.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    P = model.P.weight.to(device)   # [n_users, k]\n",
    "    Q = model.Q.weight.to(device)   # [n_items, k]\n",
    "\n",
    "    if users_idx is None:\n",
    "        users_idx = range(P.shape[0])\n",
    "\n",
    "    rows = []\n",
    "    for u in users_idx:\n",
    "        uvec = P[u]                          # [k]\n",
    "        scores = torch.mv(Q, uvec)           # [n_items]\n",
    "        vals, idxs = torch.topk(scores, min(K, Q.shape[0]))\n",
    "        uid = all_users[u]\n",
    "        idxs = idxs.cpu().numpy(); vals = vals.cpu().numpy()\n",
    "        rows.extend((uid, all_items[i], float(s)) for i, s in zip(idxs, vals))\n",
    "    return pd.DataFrame(rows, columns=[\"user_id\",\"item_id\",\"base_score\"])\n",
    "\n",
    "cand_val  = topk_candidates_streaming(model, K=200)\n",
    "cand_test = cand_val.copy()  # or regenerate after retraining on train+val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@10': 0.0081,\n",
       " 'ndcg@10': 0.0053,\n",
       " 'recall@20': 0.0134,\n",
       " 'ndcg@20': 0.0071}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "def build_gt(df):\n",
    "    gt = defaultdict(set)\n",
    "    for u,i in zip(df.user_id, df.gmap_id): gt[u].add(i)\n",
    "    return gt\n",
    "\n",
    "def recall_ndcg(cand_df, gt, K=10):\n",
    "    recs = cand_df.groupby(\"user_id\")[\"item_id\"].apply(list)\n",
    "    r_list, n_list = [], []\n",
    "    for u, items in recs.items():\n",
    "        hits = [1 if it in gt.get(u,set()) else 0 for it in items[:K]]\n",
    "        if not hits: continue\n",
    "        rec = sum(hits)/min(K, max(1,len(gt.get(u,set()))))\n",
    "        dcg = sum(h/math.log2(idx+2) for idx,h in enumerate(hits))\n",
    "        idcg = sum(1/math.log2(i+2) for i in range(min(K, len(gt.get(u,set())))))\n",
    "        ndcg = dcg/(idcg or 1)\n",
    "        r_list.append(rec); n_list.append(ndcg)\n",
    "    return float(np.mean(r_list or [0])), float(np.mean(n_list or [0]))\n",
    "\n",
    "gt_val = build_gt(val_df)\n",
    "r10, n10 = recall_ndcg(cand_val, gt_val, K=10)\n",
    "r20, n20 = recall_ndcg(cand_val, gt_val, K=20)\n",
    "{\"recall@10\":round(r10,4), \"ndcg@10\":round(n10,4), \"recall@20\":round(r20,4), \"ndcg@20\":round(n20,4)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
