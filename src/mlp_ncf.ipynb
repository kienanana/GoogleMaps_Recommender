{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e244b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d028ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_in_chunks(file_path, chunk_size=10000):\n",
    "    \"\"\"Read large JSON file in chunks\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(file):\n",
    "            chunk.append(line)\n",
    "            \n",
    "            if (i + 1) % chunk_size == 0:\n",
    "                chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
    "                chunks.append(chunk_df)\n",
    "                chunk = []  \n",
    "        \n",
    "        # process remaining lines\n",
    "        if chunk:\n",
    "            chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
    "            chunks.append(chunk_df)\n",
    "    \n",
    "    return pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ec8942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:11: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:17: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n",
      "/var/folders/hp/69k970sx1pd5lx8g7p9nmhfc0000gn/T/ipykernel_94982/3573781085.py:17: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  chunk_df = pd.read_json('\\n'.join(chunk), lines=True)\n"
     ]
    }
   ],
   "source": [
    "reviews_file_path = \"../data/processed/sf/sampled/sf-sampled-reviews.json\"\n",
    "reviews_df = read_json_in_chunks(reviews_file_path)\n",
    "restaurants_file_path = \"../data/processed/sf/sf-restaurants.json\"\n",
    "restaurants_df = read_json_in_chunks(restaurants_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7adea361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411496, 6)\n",
      "(3721, 15)\n"
     ]
    }
   ],
   "source": [
    "print(reviews_df.shape)\n",
    "print(restaurants_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f0b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 411496 entries, 0 to 411495\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   user_id       411496 non-null  float64\n",
      " 1   name          411496 non-null  object \n",
      " 2   time          411496 non-null  int64  \n",
      " 3   rating        411496 non-null  int64  \n",
      " 4   text          220550 non-null  object \n",
      " 5   gmap_id       411496 non-null  object \n",
      " 6   user_encoded  411496 non-null  int64  \n",
      " 7   item_encoded  411496 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 25.1+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users (U): 131972\n",
      "Items (I): 3721\n",
      "Interactions (N): 411496\n",
      "Data Sparsity: 0.08%\n"
     ]
    }
   ],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "restaurant_encoder = LabelEncoder()\n",
    "\n",
    "reviews_df[\"user_encoded\"] = user_encoder.fit_transform(reviews_df[\"user_id\"])\n",
    "reviews_df[\"restaurant_encoded\"] = restaurant_encoder.fit_transform(reviews_df[\"gmap_id\"])\n",
    "\n",
    "reviews_df[\"label\"] = reviews_df[\"rating\"]\n",
    "reviews_df[\"label_binary\"] = (reviews_df[\"rating\"] >= 4).astype(int)\n",
    "\n",
    "## Dataset Sparsity () counts-only\n",
    "U = reviews_df[\"user_encoded\"].nunique()\n",
    "I = reviews_df[\"restaurant_encoded\"].nunique()\n",
    "N = len(reviews_df)\n",
    "sparsity = N / (U * I)\n",
    "\n",
    "reviews_df['timestamp'] = pd.to_datetime(reviews_df['time'])\n",
    "reviews_df = reviews_df.sort_values('timestamp')\n",
    "\n",
    "split_idx = int(len(reviews_df) * 0.8)\n",
    "train_df = reviews_df.iloc[:split_idx]\n",
    "test_df = reviews_df.iloc[split_idx:]\n",
    "\n",
    "print(f\"Users (U): {U}\")\n",
    "print(f\"Restaurants (I): {I}\")\n",
    "print(f\"Interactions (N): {N}\")\n",
    "print(f\"Data Sparsity: {sparsity*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[256, 512, 256, 64]\n",
    "dropout_sizes = [0.2,0.3,0.4,0.3,0.2]\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"A reusable fully-connected block with optional BatchNorm, ReLU, and Dropout.\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.0, use_bn=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, out_dim)]\n",
    "        if use_bn:\n",
    "            layers.append(nn.BatchNorm1d(out_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_restaurants, embedding_dim=64):\n",
    "        super(NCFModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_restaurants, embedding_dim)\n",
    "\n",
    "        mlp_layers = []\n",
    "        input_dim = embedding_dim * 2\n",
    "        for i in range(len(layer_sizes)):\n",
    "            hidden_dim = layer_sizes[i]\n",
    "            dropout = dropout_sizes[i]\n",
    "            mlp_layers.append(MLPBlock(input_dim, hidden_dim, dropout=dropout))\n",
    "            input_dim = hidden_dim\n",
    "        mlp_layers.append(nn.Linear(input_dim, 1))  # output layer\n",
    "        self.mlp=nn.Sequential(*mlp_layers)\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_embedding(user)\n",
    "        i = self.item_embedding(item)\n",
    "        x = torch.cat([u, i], dim=1)\n",
    "        logits = self.mlp(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a627c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        # Convert columns to tensors\n",
    "        self.users = torch.tensor(dataframe[\"user_encoded\"].values, dtype=torch.long)\n",
    "        self.restaurants = torch.tensor(dataframe[\"restaurant_encoded\"].values, dtype=torch.long)\n",
    "        self.labels = torch.tensor(dataframe[\"label\"].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.restaurants[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ReviewsDataset(train_df)\n",
    "test_dataset = ReviewsDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 512, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1028, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dca530b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131972 3721\n"
     ]
    }
   ],
   "source": [
    "num_users = reviews_df[\"user_encoded\"].nunique()\n",
    "num_restaurants = reviews_df[\"restaurant_encoded\"].nunique()\n",
    "print(num_users, num_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCFModel(\n",
      "  (user_embedding): Embedding(131972, 64)\n",
      "  (item_embedding): Embedding(3721, 64)\n",
      "  (mlp): Sequential(\n",
      "    (0): MLPBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): MLPBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): MLPBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): MLPBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): MLPBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "====================================================================================\n",
      "Epoch 1:\n",
      "Training Loss: 13.0258, Validation Loss: 9.3050\n",
      "Training RMSE: 3.6091, Validation RMSE: 3.0504\n",
      "Validation Accuracy: 14.3840%\n",
      "====================================================================================\n",
      "Epoch 2:\n",
      "Training Loss: 4.3951, Validation Loss: 3.8741\n",
      "Training RMSE: 2.0964, Validation RMSE: 1.9683\n",
      "Validation Accuracy: 14.3840%\n",
      "====================================================================================\n",
      "Epoch 3:\n",
      "Training Loss: 1.7266, Validation Loss: 1.8453\n",
      "Training RMSE: 1.3140, Validation RMSE: 1.3584\n",
      "Validation Accuracy: 16.6731%\n",
      "====================================================================================\n",
      "Epoch 4:\n",
      "Training Loss: 1.3683, Validation Loss: 1.3713\n",
      "Training RMSE: 1.1697, Validation RMSE: 1.1710\n",
      "Validation Accuracy: 33.9307%\n",
      "====================================================================================\n",
      "Epoch 5:\n",
      "Training Loss: 1.3031, Validation Loss: 1.1712\n",
      "Training RMSE: 1.1415, Validation RMSE: 1.0822\n",
      "Validation Accuracy: 51.6598%\n",
      "====================================================================================\n",
      "Epoch 6:\n",
      "Training Loss: 1.2257, Validation Loss: 1.1166\n",
      "Training RMSE: 1.1071, Validation RMSE: 1.0567\n",
      "Validation Accuracy: 60.8299%\n",
      "====================================================================================\n",
      "Epoch 7:\n",
      "Training Loss: 1.1078, Validation Loss: 1.1347\n",
      "Training RMSE: 1.0525, Validation RMSE: 1.0652\n",
      "Validation Accuracy: 62.6646%\n",
      "====================================================================================\n",
      "Epoch 8:\n",
      "Training Loss: 0.9836, Validation Loss: 1.0764\n",
      "Training RMSE: 0.9918, Validation RMSE: 1.0375\n",
      "Validation Accuracy: 70.0790%\n",
      "====================================================================================\n",
      "Epoch 9:\n",
      "Training Loss: 0.8734, Validation Loss: 1.0954\n",
      "Training RMSE: 0.9346, Validation RMSE: 1.0466\n",
      "Validation Accuracy: 72.2600%\n",
      "====================================================================================\n",
      "Epoch 10:\n",
      "Training Loss: 0.8384, Validation Loss: 1.1034\n",
      "Training RMSE: 0.9157, Validation RMSE: 1.0504\n",
      "Validation Accuracy: 71.3208%\n",
      "====================================================================================\n",
      "Epoch 11:\n",
      "Training Loss: 0.8092, Validation Loss: 1.0965\n",
      "Training RMSE: 0.8996, Validation RMSE: 1.0471\n",
      "Validation Accuracy: 72.7145%\n",
      "====================================================================================\n",
      "Epoch 12:\n",
      "Training Loss: 0.7781, Validation Loss: 1.1127\n",
      "Training RMSE: 0.8821, Validation RMSE: 1.0549\n",
      "Validation Accuracy: 72.7193%\n"
     ]
    }
   ],
   "source": [
    "model = NCFModel(num_users, num_restaurants)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    total_samples = 0\n",
    "    for u, i, r in train_loader:\n",
    "        u, i, r = u.to(device), i.to(device), r.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(u, i).squeeze(-1)\n",
    "\n",
    "        loss = criterion(logits, r)\n",
    "        training_loss += loss.item() * r.size(0)\n",
    "        total_samples += r.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_training_loss = training_loss / total_samples\n",
    "    train_losses.append(average_training_loss)\n",
    "    training_rmse = np.sqrt(average_training_loss)\n",
    "\n",
    "    if average_training_loss < 1.0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 2e-5  \n",
    "\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    val_loss = 0\n",
    "    all_preds, all_labels =  [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u, i, r in test_loader:\n",
    "            u, i, r = u.to(device), i.to(device), r.to(device)\n",
    "            logits = model(u,i).squeeze(-1)\n",
    "\n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_labels.extend(r.cpu().numpy())\n",
    "\n",
    "            predictions = (logits >= 4).int()\n",
    "            actual = (r>= 4).int()\n",
    "            new_correct = (predictions == actual).sum().item()\n",
    "            num_correct += new_correct\n",
    "\n",
    "            loss = criterion(logits, r)\n",
    "            val_loss += loss.item()*r.size(0)\n",
    "\n",
    "    average_val_loss = val_loss/len(all_preds)\n",
    "    val_rmse = np.sqrt(average_val_loss)\n",
    "    val_losses.append(average_val_loss)\n",
    "    accuracy = num_correct / len(all_labels)\n",
    "\n",
    "\n",
    "    print(\"====================================================================================\")\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"Training Loss: {average_training_loss:.4f}, Validation Loss: {average_val_loss:.4f}\")\n",
    "    print(f\"Training RMSE: {training_rmse:.4f}, Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Validation Accuracy: {100*accuracy:.4f}%\")\n",
    "\n",
    "\n",
    "# Plotting of training and validation loss curves\n",
    "plt.plot(range(1, 21), train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, 21), val_losses, label='Validation Loss', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9b1ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'name', 'time', 'rating', 'text', 'gmap_id', 'user_encoded',\n",
       "       'restaurant_encoded', 'label', 'label_binary', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd05cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_ids_test = test_df['restaurant_encoded'].unique()\n",
    "restaurant_ids_train = train_df['restaurant_encoded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec5e1e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589\n",
      "3107\n"
     ]
    }
   ],
   "source": [
    "print(len(restaurant_ids_train))\n",
    "print(len(restaurant_ids_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842f6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_binary\n",
       "1    271255\n",
       "0     57941\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label_binary'].value_counts()\n",
    "#total: 329196\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "031e9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"first_ncf_recommender.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac86196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
